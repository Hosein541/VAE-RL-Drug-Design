{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### De Novo Design of EGFR Inhibitors using a Deep Generative Model\n",
        "\n",
        "* **Author:** Hosein Mohammadi\n",
        "* **Date:** July 2024\n",
        "* **Contact:** [huseinmohammadi83@gmail.com](mailto:huseinmohammadi83@gmail.com)\n",
        "* **LinkedIn:** [Hosein Mohammadi](https://www.linkedin.com/in/hosein-mohammadi-979b8a2b2/)\n",
        "* **Project Repository:** [VAE-RL-Drug-Design](https://github.com/Hosein541/VAE-RL-Drug-Design)\n",
        "---\n",
        "\n",
        "### 1. Project Overview\n",
        "\n",
        "This notebook implements an end-to-end pipeline for de novo drug design, a key process in computational chemistry and drug discovery. The primary objective is to develop and optimize a deep generative model capable of designing novel molecular structures. These molecules are specifically targeted to inhibit the **Epidermal Growth Factor Receptor (EGFR) kinase domain**, a crucial target in modern cancer therapy.\n",
        "\n",
        "The project leverages a series of advanced techniques to ensure the generation of valid, unique, and drug-like molecules with desirable properties.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Environment Setup & Library Imports\n",
        "\n",
        "**Objective:**\n",
        "This initial cell prepares the notebook's environment. It installs all required external libraries and then imports all necessary modules that will be used throughout the project.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Installation:** The `pip` command is used to install three core external libraries:\n",
        "    * `chembl_webresource-client`: To programmatically access and download data from the ChEMBL database.\n",
        "    * `rdkit-pypi`: The primary toolkit for cheminformatics, used for processing molecular structures and calculating properties.\n",
        "    * `selfies`: A library for the SELFIES (SELF-referencIng Embedded Strings) representation of molecules, which guarantees 100% valid structures during generation.\n",
        "\n",
        "2.  **Imports:** The required modules are imported and grouped by category: standard Python libraries, data science/visualization tools, cheminformatics libraries, and the PyTorch deep learning framework.\n",
        "\n",
        "3.  **SA_Score Configuration:** The path to RDKit's Synthetic Accessibility (SA) score module is explicitly added to the system path to ensure it can be imported and used.\n",
        "\n",
        "**Outcome:**\n",
        "Upon successful execution, all dependencies are loaded, and a confirmation message is printed. The notebook is now ready for the data acquisition and processing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FARuBlM1Z0C7"
      },
      "outputs": [],
      "source": [
        "# install neccesary libraries\n",
        "!pip install chembl_webresource-client\n",
        "!pip install rdkit-pypi\n",
        "!pip install selfies\n",
        "\n",
        "\n",
        "# Section 1: Standard Python Libraries\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Section 2: Core (Third-party) Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import selfies as sf\n",
        "\n",
        "# --- Chemoinformatics Libraries (RDKit & ChEMBL) ---\n",
        "from chembl_webresource_client.new_client import new_client\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Descriptors, QED, RDConfig\n",
        "# Add SA_Score module to Python path for synthesis accessibility calculation\n",
        "sys.path.append(os.path.join(RDConfig.RDContribDir, 'SA_Score'))\n",
        "import sascorer\n",
        "\n",
        "\n",
        "# Section 3: Deep Learning Libraries (PyTorch)\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "print(\"✅ All required libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Data Acquisition and Preprocessing from ChEMBL\n",
        "\n",
        "**Objective:**\n",
        "This cell downloads, cleans, and processes the bioactivity data for our target protein, EGFR, from the ChEMBL database. The goal is to create a clean dataset of molecules and their corresponding inhibitory potencies (IC50 values) that will be used to train the generative model.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Data Retrieval:** The code connects to the ChEMBL database using its API client. It targets the **Epidermal Growth Factor Receptor (EGFR)** using its unique ChEMBL ID, `CHEMBL203`. It then fetches up to 30,000 associated activity records, specifically requesting the molecule's structure (`canonical_smiles`) and its `standard_type` and `standard_value`.\n",
        "2.  **Data Cleaning & Filtering:** The raw data is converted into a pandas DataFrame and undergoes several crucial cleaning steps:\n",
        "    * The dataset is filtered to keep only records where the activity is measured as **IC50**, a standard measure of a drug's potency.\n",
        "    * Any rows that have missing values for the IC50 or the SMILES string are removed to ensure data quality.\n",
        "    * All IC50 values are ensured to be positive numbers, as required for the subsequent logarithmic conversion.\n",
        "3.  **Feature Engineering (pIC50):** A new feature, **pIC50**, is calculated from the IC50 values. The pIC50 is the negative base-10 logarithm of the IC50 value (in Molar concentration). This transformation is standard practice as it converts the data to a more convenient logarithmic scale where higher values indicate greater potency, making it more suitable for machine learning models.\n",
        "\n",
        "**Outcome:**\n",
        "The final, cleaned dataset is saved to `egfr_ic50_dataset.csv`. This file contains the essential information for each molecule—its SMILES string, IC50 value, and the calculated pIC50—and serves as the foundational dataset for the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5EdCpEhPDGU",
        "outputId": "bad31a10-ce0a-4916-c22d-59e2508bd802"
      },
      "outputs": [],
      "source": [
        "# --- Settings ---\n",
        "target_id = \"CHEMBL203\"\n",
        "max_records = 30000\n",
        "\n",
        "activity = new_client.activity\n",
        "\n",
        "print(\"⏳ Fetching data from ChEMBL...\")\n",
        "\n",
        "# EGFR data generator\n",
        "activity_gen = activity.filter(\n",
        "    target_chembl_id=target_id\n",
        ").only(\n",
        "    [\n",
        "        \"canonical_smiles\",\n",
        "        \"standard_type\",\n",
        "        \"standard_value\",\n",
        "        \"molecule_chembl_id\",\n",
        "        \"activity_id\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Manually retrieve data up to max_records\n",
        "records = []\n",
        "for i, record in enumerate(activity_gen):\n",
        "    if i >= max_records:\n",
        "        break\n",
        "    records.append(record)\n",
        "    if i % 500 == 0:\n",
        "        print(f\"🔄 Received record {i}\")\n",
        "\n",
        "print(f\"🎯 Number of records received: {len(records)}\")\n",
        "\n",
        "# --- Convert to DataFrame and Preprocessing ---\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "# Filter IC50 and numerical values\n",
        "df = df[df[\"standard_type\"] == \"IC50\"]\n",
        "df = df[df[\"standard_value\"].notna()]\n",
        "df = df[df[\"canonical_smiles\"].notna()]\n",
        "df[\"standard_value\"] = pd.to_numeric(df[\"standard_value\"], errors=\"coerce\")\n",
        "df = df[df[\"standard_value\"] > 0]\n",
        "\n",
        "# Calculate pIC50\n",
        "df[\"pIC50\"] = -np.log10(df[\"standard_value\"] * 1e-9)\n",
        "\n",
        "# Final output\n",
        "final_df = df[[\"molecule_chembl_id\", \"canonical_smiles\", \"standard_value\", \"pIC50\"]]\n",
        "final_df.columns = [\"chembl_id\", \"smiles\", \"IC50_nM\", \"pIC50\"]\n",
        "final_df.to_csv(\"egfr_ic50_dataset.csv\", index=False)\n",
        "\n",
        "print(\"✅ File egfr_ic50_dataset.csv saved.\")\n",
        "print(f\"📦 Final records: {final_df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Data Preparation and SMILES Tokenization\n",
        "\n",
        "**Objective:**\n",
        "This cell takes the processed dataset and prepares the SMILES strings for input into the neural network. This involves creating a character-level vocabulary, converting the strings into sequences of integer tokens, and padding them to a uniform length to be batched into a tensor.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Data Loading:** The `egfr_ic50_dataset.csv` file is loaded into a pandas DataFrame, and the `smiles` column is extracted into a list. This list is also saved to `train.smi`, a common file format for molecular datasets.\n",
        "2.  **Vocabulary Creation:** A character-level vocabulary is built by finding all unique characters present across all SMILES strings in the dataset. Four special tokens are added to this vocabulary:\n",
        "    * `<PAD>`: For padding sequences to the same length.\n",
        "    * `<START>`: To signify the beginning of a sequence.\n",
        "    * `<END>`: To signify the end of a sequence.\n",
        "    * `<UNK>`: To represent any unknown characters.\n",
        "    \n",
        "    Mappings from characters to integer indices (`char2idx`) and vice-versa (`idx2char`) are then created.\n",
        "3.  **Tokenization & Padding:** Each SMILES string is converted into a numerical sequence. This is done by:\n",
        "    * Adding `<START>` and `<END>` tokens to the beginning and end of each string.\n",
        "    * Mapping every character to its corresponding integer index from the `char2idx` dictionary.\n",
        "    * Padding every sequence with the `<PAD>` token's index until it reaches the `max_length` (defined by the longest molecule in the dataset). This ensures all input sequences have a uniform size, which is necessary for batch processing in PyTorch.\n",
        "\n",
        "**Outcome:**\n",
        "The final output is `encoded_tensor`, a PyTorch LongTensor. Each row in this tensor represents a molecule, and the numbers correspond to the character indices in the vocabulary. This tensor is the final prepared input, ready to be used for training the generative model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF6IEmINRp9Z",
        "outputId": "492a1181-15e3-4fd5-9081-b954d55dad0b"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"egfr_ic50_dataset.csv\")\n",
        "\n",
        "# We only use the SMILES column for reconstruction\n",
        "smiles_list = df['smiles'].tolist()\n",
        "print(f\"Total SMILES count: {len(smiles_list)}\")\n",
        "\n",
        "with open('train.smi', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(smiles_list))\n",
        "\n",
        "\n",
        "# Define special tokens\n",
        "special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']\n",
        "\n",
        "# Extract all unique characters from SMILES\n",
        "charset = set(''.join(smiles_list))\n",
        "charset = sorted(list(charset))\n",
        "\n",
        "# Build the final vocabulary\n",
        "vocab = special_tokens + charset\n",
        "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Total number of characters (vocab size): {vocab_size}\")\n",
        "\n",
        "\n",
        "# Set maximum length (e.g., based on the 95th percentile of lengths)\n",
        "max_length = max(len(s) for s in smiles_list) + 2  # +2 for <START> and <END>\n",
        "print(f\"Selected sequence length: {max_length}\")\n",
        "\n",
        "def tokenize_smiles(smi):\n",
        "    tokens = ['<START>'] + list(smi) + ['<END>']\n",
        "    idxs = [char2idx.get(ch, char2idx['<UNK>']) for ch in tokens]\n",
        "    # Padding\n",
        "    padding = [char2idx['<PAD>']] * (max_length - len(idxs))\n",
        "    return idxs + padding\n",
        "\n",
        "encoded_smiles = [tokenize_smiles(smi) for smi in smiles_list]\n",
        "encoded_tensor = torch.tensor(encoded_smiles, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. VAE Model Training with Curriculum Learning\n",
        "\n",
        "**Objective:**\n",
        "This cell defines and trains the core generative model, a Variational Autoencoder (VAE) based on SELFIES. It employs a **Curriculum Learning** strategy to guide the model toward generating more desirable, drug-like molecules.\n",
        "\n",
        "**Methodology:**\n",
        "This process is divided into several logical parts within the code:\n",
        "\n",
        "1.  **Configuration:** Key hyperparameters for the model and training process are defined, including learning rate (`LR`), batch size (`BATCH_SIZE`), and the number of epochs for pre-training (`PRE_EPOCHS`) and curriculum fine-tuning (`CURR_EPOCHS`).\n",
        "2.  **Helper Classes & Functions:**\n",
        "    * `SelfiesTokenizer`: Converts SELFIES strings into integer tensors for the model.\n",
        "    * `SelfiesDataset`: A PyTorch `Dataset` class to handle the data loading.\n",
        "    * `VAE`: The neural network architecture, featuring a bidirectional GRU encoder and a GRU decoder. It learns a compressed latent space representation (`z`) of the molecules.\n",
        "    * `loss_fn`: Calculates the VAE's loss, which is a combination of the reconstruction loss (how well the model can recreate an input molecule) and the KL divergence (which regularizes the latent space).\n",
        "    * `keep`: A helper function that defines our criteria for a \"high-quality\" molecule based on QED, SA score, and molecular weight (MW).\n",
        "3.  **Training Workflow (Curriculum Learning):**\n",
        "    * **Pre-training:** The VAE model is first trained for `PRE_EPOCHS` on the **entire dataset**. This allows the model to learn the general grammar and patterns of the whole chemical space.\n",
        "    * **Filtering:** The dataset is then filtered using the `keep` function, creating a smaller, higher-quality subset of molecules that meet our drug-like criteria.\n",
        "    * **Fine-tuning:** The pre-trained model is then trained for an additional `CURR_EPOCHS` exclusively on this **filtered dataset**. This second stage fine-tunes the model, biasing it to generate new molecules that share the desirable properties of the high-quality subset.\n",
        "\n",
        "**Outcome:**\n",
        "The cell trains the VAE model and saves its learned weights and vocabulary to a file named `vae_curriculum.pt`. This model is now capable of generating novel, valid molecules and is primed to produce a higher proportion of drug-like compounds compared to a model trained without curriculum learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HXnmOQfRP55e",
        "outputId": "8c1fc17a-2757-4bdf-f36f-dc8d6ad9535a"
      },
      "outputs": [],
      "source": [
        "PRE_EPOCHS, CURR_EPOCHS = 15, 35\n",
        "BATCH_SIZE, EMB, HID, LAT = 512, 128, 256, 128\n",
        "LR, FREE_BITS = 1e-3, 1.0\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "QED_MIN, SA_MAX, MW_MIN, MW_MAX = 0.5, 5.8, 150, 600\n",
        "\n",
        "class SelfiesTokenizer:\n",
        "    def __init__(self, selfies: List[str]):\n",
        "        specials = [\"<PAD>\", \"<BOS>\", \"<EOS>\"]\n",
        "        vocab = sorted({t for s in selfies for t in sf.split_selfies(s)})\n",
        "        self.idx2tok = specials + vocab\n",
        "        self.tok2idx = {t: i for i, t in enumerate(self.idx2tok)}\n",
        "        self.pad = self.tok2idx[\"<PAD>\"]\n",
        "        self.bos = self.tok2idx[\"<BOS>\"]\n",
        "        self.eos = self.tok2idx[\"<EOS>\"]\n",
        "        self.pad_idx = self.pad\n",
        "        self.bos_idx = self.bos\n",
        "        self.eos_idx = self.eos\n",
        "    def encode(self, s: str) -> torch.LongTensor:\n",
        "        ids = [self.bos] + [self.tok2idx[t] for t in sf.split_selfies(s)] + [self.eos]\n",
        "        return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "class SelfiesDataset(Dataset):\n",
        "    def __init__(self, selfies: List[str], tok: SelfiesTokenizer):\n",
        "        self.data, self.tok = selfies, tok\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i): return self.tok.encode(self.data[i])\n",
        "\n",
        "def collate(batch, pad):\n",
        "    batch = [torch.as_tensor(x) for x in batch]\n",
        "    lens = torch.tensor([len(x) for x in batch])\n",
        "    padded = pad_sequence(batch, batch_first=True, padding_value=pad)\n",
        "    return padded[:, :-1], padded[:, 1:], lens - 1\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, vocab, pad, bos):\n",
        "        super().__init__()\n",
        "        self.pad, self.bos = pad, bos\n",
        "        self.emb = nn.Embedding(vocab, EMB, padding_idx=pad)\n",
        "        self.encoder = nn.GRU(EMB, HID, 2, batch_first=True, bidirectional=True, dropout=0.3)\n",
        "        self.mu = nn.Linear(HID * 2, LAT)\n",
        "        self.logvar = nn.Linear(HID * 2, LAT)\n",
        "        self.decoder = nn.GRU(EMB + LAT, HID, 2, batch_first=True, dropout=0.3)\n",
        "        self.out = nn.Linear(HID, vocab)\n",
        "        self.out_proj = self.out\n",
        "    def encode(self, x, l):\n",
        "        _, h = self.encoder(nn.utils.rnn.pack_padded_sequence(self.emb(x), l.cpu(), batch_first=True, enforce_sorted=False))\n",
        "        h = torch.cat([h[-2], h[-1]], -1)\n",
        "        return self.mu(h), self.logvar(h)\n",
        "    @staticmethod\n",
        "    def reparam(mu, logvar):\n",
        "        return mu + torch.randn_like(logvar) * torch.exp(0.5 * logvar)\n",
        "    def decode(self, z, tgt, tfr):\n",
        "        B, T = tgt.shape\n",
        "        outputs, hidden = [], None\n",
        "        tok = torch.full((B,), self.bos, device=tgt.device, dtype=torch.long)\n",
        "        for t in range(T):\n",
        "            dec_in = torch.cat([self.emb(tok), z], -1).unsqueeze(1)\n",
        "            out, hidden = self.decoder(dec_in, hidden)\n",
        "            logits = self.out(out.squeeze(1))\n",
        "            logits[:, self.bos] = -1e9\n",
        "            outputs.append(logits)\n",
        "            tok = torch.where(torch.rand(B, device=tgt.device) < tfr, tgt[:, t], logits.argmax(-1))\n",
        "        return torch.stack(outputs, 1)\n",
        "    def forward(self, inp, tgt, l, tfr):\n",
        "        mu, logvar = self.encode(inp, l)\n",
        "        z = self.reparam(mu, logvar)\n",
        "        return self.decode(z, tgt, tfr), mu, logvar\n",
        "\n",
        "def kl_div(mu, logvar): return -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(1)\n",
        "def cyc_beta(step, total, cycles=4, max_beta=4.0):\n",
        "    return max_beta * ((step % (total // cycles)) / (total // cycles))\n",
        "def loss_fn(logits, tgt, mu, logvar, beta, pad):\n",
        "    recon = F.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt.reshape(-1), ignore_index=pad)\n",
        "    kl = kl_div(mu, logvar).clamp(min=FREE_BITS).mean()\n",
        "    return recon + beta * kl, recon, kl\n",
        "\n",
        "def keep(smi: str):\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol is None: return False\n",
        "    q, sa, mw = QED.qed(mol), sascorer.calculateScore(mol), Descriptors.MolWt(mol)\n",
        "    return q >= QED_MIN and sa <= SA_MAX and MW_MIN <= mw <= MW_MAX\n",
        "\n",
        "def train_epoch(model, loader, opt, step, total):\n",
        "    model.train(); r_sum = k_sum = 0\n",
        "    for inp, tgt, lens in loader:\n",
        "        inp, tgt, lens = inp.to(DEVICE), tgt.to(DEVICE), lens.to(DEVICE)\n",
        "        beta = cyc_beta(step, total)\n",
        "        tfr = max(0.3, 1 - step / (0.25 * total))\n",
        "        logits, mu, logvar = model(inp, tgt, lens, tfr)\n",
        "        loss, recon, kl = loss_fn(logits, tgt, mu, logvar, beta, model.pad)\n",
        "        opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "        r_sum += recon.item(); k_sum += kl.item(); step += 1\n",
        "    return r_sum / len(loader), k_sum / len(loader), step\n",
        "\n",
        "def sample(model, tokenizer, *, max_len=150, n=32, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n, LAT, device=device)\n",
        "        seq = torch.full((n, 1), tokenizer.bos_idx, dtype=torch.long, device=device)\n",
        "        hidden = None\n",
        "        for _ in range(max_len):\n",
        "            dec_in = torch.cat([model.emb(seq[:, -1]), z], -1).unsqueeze(1)\n",
        "            dec_out, hidden = model.decoder(dec_in, hidden)\n",
        "            logits = model.out_proj(dec_out.squeeze(1))\n",
        "            logits[:, tokenizer.bos_idx] = -1e9\n",
        "            next_tok = torch.multinomial(F.softmax(logits, -1), 1)\n",
        "            seq = torch.cat([seq, next_tok], 1)\n",
        "            if (next_tok == tokenizer.eos_idx).all(): break\n",
        "    selfies = [\"\".join(tokenizer.idx2tok[i] for i in s[1:] if i not in (tokenizer.pad_idx, tokenizer.eos_idx)) for s in seq]\n",
        "    return [sf.decoder(se) if sf.is_valid_selfies(se) else \"\" for se in selfies]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    src = Path(\"train.smi\"); assert src.exists()\n",
        "    smiles = [l.strip() for l in src.read_text().splitlines() if l.strip() and Chem.MolFromSmiles(l.strip())]\n",
        "    selfies_all = [sf.encoder(s) for s in smiles]\n",
        "    tok = SelfiesTokenizer(selfies_all)\n",
        "    loader_all = DataLoader(SelfiesDataset(selfies_all, tok), BATCH_SIZE, True, collate_fn=lambda b: collate(b, tok.pad))\n",
        "    model = VAE(len(tok.idx2tok), tok.pad, tok.bos).to(DEVICE)\n",
        "    opt = optim.Adam(model.parameters(), lr=LR)\n",
        "    total_steps, step = (PRE_EPOCHS + CURR_EPOCHS) * len(loader_all), 0\n",
        "    for e in range(1, PRE_EPOCHS + 1):\n",
        "        r, k, step = train_epoch(model, loader_all, opt, step, total_steps)\n",
        "        print(f\"[PRE {e}/{PRE_EPOCHS}] recon {r:.3f} kl {k:.3f}\")\n",
        "    smiles_filt = [s for s in smiles if keep(s)]\n",
        "    Path(\"filtered.smi\").write_text(\"\\n\".join(smiles_filt))\n",
        "    selfies_filt = [sf.encoder(s) for s in smiles_filt]\n",
        "    loader_filt = DataLoader(SelfiesDataset(selfies_filt, tok), BATCH_SIZE, True, collate_fn=lambda b: collate(b, tok.pad))\n",
        "    for e in range(1, CURR_EPOCHS + 1):\n",
        "        r, k, step = train_epoch(model, loader_filt, opt, step, total_steps)\n",
        "        print(f\"[CURR {e}/{CURR_EPOCHS}] recon {r:.3f} kl {k:.3f}\")\n",
        "    torch.save({\"state_dict\": model.state_dict(), \"vocab\": tok.idx2tok}, \"vae_curriculum.pt\")\n",
        "    print(\"✅ training complete; model saved → vae_curriculum.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Molecule Generation and Property Evaluation\n",
        "\n",
        "**Objective:**\n",
        "This cell uses the trained VAE model (after curriculum learning) to generate a large library of new, unseen molecules. It then evaluates the chemical properties of these generated molecules to assess the model's performance and create a dataset for further analysis.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Sampling Function:** The `sample` function is defined to generate new molecules. It works by:\n",
        "    * Sampling random points (`z`) from the model's latent space.\n",
        "    * Using the VAE's decoder to generate new sequences of SELFIES tokens, starting from a `<BOS>` (Beginning of Sequence) token.\n",
        "    * The generation is done autoregressively, where the token predicted at each step is fed as the input for the next step.\n",
        "    * Finally, the generated SELFIES strings are decoded into their corresponding SMILES representations.\n",
        "2.  **Generation & Evaluation:**\n",
        "    * A large batch of `N_SAMPLES` (5,000) molecules is generated by calling the `sample` function.\n",
        "    * The code then iterates through the list of generated SMILES strings. Each valid SMILES is converted into an RDKit molecule object.\n",
        "    * For each valid molecule, three key medicinal chemistry properties are calculated: **QED** (Quantitative Estimation of Drug-likeness), **SA Score** (Synthetic Accessibility), and **MW** (Molecular Weight).\n",
        "    * These properties, along with the SMILES string, are stored in a pandas DataFrame.\n",
        "\n",
        "**Outcome:**\n",
        "A DataFrame containing 4,989 valid molecules and their calculated properties is created. The descriptive statistics (mean, std, etc.) of these properties are printed to provide an initial assessment of the generated chemical space. The entire dataset is then saved to `generated_molecules.csv` for the next stage of filtering and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWt4UOpWRp1f"
      },
      "outputs": [],
      "source": [
        "# ---------- sampling ----------\n",
        "def sample(model, tokenizer, *, max_len=150, n=32, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n, model.mu.out_features, device=device)\n",
        "        seq = torch.full((n, 1), tokenizer.bos, dtype=torch.long, device=device)\n",
        "        hidden = None\n",
        "        for _ in range(max_len):\n",
        "            dec_in  = torch.cat([model.emb(seq[:, -1]), z], dim=-1).unsqueeze(1)\n",
        "            dec_out, hidden = model.decoder(dec_in, hidden)\n",
        "            logits  = model.out_proj(dec_out.squeeze(1))   # alias set in VAE\n",
        "            logits[:, tokenizer.bos] = -1e9\n",
        "            next_tok = torch.multinomial(torch.softmax(logits, dim=-1), 1)\n",
        "            seq = torch.cat([seq, next_tok], dim=1)\n",
        "            if (next_tok == tokenizer.eos).all():\n",
        "                break\n",
        "\n",
        "    selfies_list = [\n",
        "        \"\".join(\n",
        "            tokenizer.idx2tok[idx]\n",
        "            for idx in s[1:]\n",
        "            if idx not in (tokenizer.pad, tokenizer.eos)\n",
        "        )\n",
        "        for s in seq\n",
        "    ]\n",
        "    smiles = []\n",
        "    for se in selfies_list:\n",
        "        try:\n",
        "            smiles.append(sf.decoder(se))\n",
        "        except sf.DecoderError:\n",
        "            smiles.append(\"\")\n",
        "    return smiles\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Sampling + QED / SA evaluation\n",
        "# ================================\n",
        "N_SAMPLES, MAX_LEN = 5000, 150\n",
        "CSV_NAME = \"generated_molecules.csv\"\n",
        "\n",
        "smiles_list = sample(model, tok, n=N_SAMPLES, max_len=MAX_LEN, device=DEVICE)\n",
        "\n",
        "records = []\n",
        "for smi in smiles_list:\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol is None:\n",
        "        continue\n",
        "    records.append(\n",
        "        {\n",
        "            \"smiles\": smi,\n",
        "            \"qed\": QED.qed(mol),\n",
        "            \"sa\": sascorer.calculateScore(mol),\n",
        "            \"mw\": Descriptors.MolWt(mol),\n",
        "        }\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "print(f\"Kept {len(df)}/{N_SAMPLES} molecules ({len(df)/N_SAMPLES:.1%})\")\n",
        "print(df[[\"qed\", \"sa\", \"mw\"]].describe())\n",
        "\n",
        "df.to_csv(CSV_NAME, index=False)\n",
        "print(f\"✅  CSV saved to {CSV_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Hit Selection and Visualization of Generated Molecules\n",
        "\n",
        "**Objective:**\n",
        "This cell filters the large library of molecules generated by the curriculum-trained VAE to identify a smaller subset of high-quality \"hits\" based on established medicinal chemistry rules. It also generates plots to visualize the property distributions of the entire generated library, providing insight into the model's performance.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Hit Identification:** A set of filtering criteria is defined to select the most promising drug-like molecules from the `generated_molecules.csv` file:\n",
        "    * **QED (Quantitative Estimation of Drug-likeness):** Must be greater than or equal to 0.6.\n",
        "    * **SA Score (Synthetic Accessibility):** Must be less than or equal to 5.0 (a lower score indicates the molecule is easier to synthesize).\n",
        "    * **Molecular Weight (MW):** Must be within the 150-600 g/mol range, consistent with Lipinski's Rule of Five for oral bioavailability.\n",
        "    \n",
        "    The DataFrame is filtered using these rules, and the resulting \"hits\" are counted and saved to a new DataFrame.\n",
        "        \n",
        "2.  **Visualization:** Three plots are generated using `matplotlib` to analyze the property space of the generated molecules:\n",
        "    * **QED & SA Score Histograms:** These plots show the frequency distribution of the drug-likeness and synthetic accessibility scores for all generated molecules. A red dashed line on each plot indicates the threshold used for filtering.\n",
        "    * **QED vs. SA Score Scatter Plot:** This plot visualizes the relationship between the two key properties for all molecules. The red lines define a \"hit region\" in the bottom-right quadrant, which contains the most desirable molecules (high QED, low SA).\n",
        "\n",
        "**Outcome:**\n",
        "The top candidate molecules that pass all filters are saved to a new file, `VAE_top_hits.csv`. The generated plots provide a clear visual summary of the model's output, showing that while it explores a broad chemical space, a specific subset of high-quality molecules can be successfully isolated using these standard filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "Rx1Esbzb5_CM",
        "outputId": "82a226b4-4ff0-4426-9ac2-7556131eae3b"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 1) Filter \"Hit\" Molecules + 2) Plot Distributions and Scatter\n",
        "#    (Assuming df is the DataFrame generated in the previous cell)\n",
        "# ==============================================================\n",
        "\n",
        "# ---------- 1. Select Hits ----------\n",
        "# Suggested criteria; adjust as desired\n",
        "QED_MIN   = 0.60          # Drug-likeness\n",
        "SA_MAX    = 5.0           # Acceptable synthetic accessibility\n",
        "MW_MIN    = 150           # Remove very light ones (often solvents or small fragments)\n",
        "MW_MAX    = 600           # According to Lipinski's Rule\n",
        "\n",
        "hits = df[\n",
        "    (df.qed >= QED_MIN) &\n",
        "    (df.sa  <= SA_MAX)  &\n",
        "    (df.mw  >= MW_MIN)  &\n",
        "    (df.mw  <= MW_MAX)\n",
        "].copy()\n",
        "\n",
        "print(f\"🎯  Found {len(hits)} high-quality molecules out of {len(df)}  \"\n",
        "      f\"({len(hits)/len(df):.1%})\")\n",
        "\n",
        "# Save Hits\n",
        "hits.to_csv(\"VAE_top_hits.csv\", index=False)\n",
        "print(\"✅  Saved to top_hits.csv\")\n",
        "\n",
        "\n",
        "# ---------- 2. Plot Distributions ----------\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "# ---QED Histogram ---\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(df.qed, bins=40)\n",
        "plt.axvline(QED_MIN, color='red', linestyle='--')\n",
        "plt.title(\"QED distribution\")\n",
        "plt.xlabel(\"QED\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# ---SA Histogram---\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(df.sa, bins=40)\n",
        "plt.axvline(SA_MAX, color='red', linestyle='--')\n",
        "plt.title(\"SA Score distribution\")\n",
        "plt.xlabel(\"SA (lower = easier)\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# --- Scatter QED vs SA ---\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(df.qed, df.sa, s=10, alpha=0.4)\n",
        "plt.axvline(QED_MIN, color='red', linestyle='--')\n",
        "plt.axhline(SA_MAX, color='red', linestyle='--')\n",
        "plt.title(\"QED vs SA\")\n",
        "plt.xlabel(\"QED (higher is better)\")\n",
        "plt.ylabel(\"SA (lower is better)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Reinforcement Learning (RL) for Targeted Molecular Optimization\n",
        "\n",
        "**Objective:**\n",
        "This cell performs the final and most crucial optimization step. It fine-tunes the previously trained VAE model using a Reinforcement Learning (RL) approach. The goal is to steer the model's generative process specifically towards molecules that not only are drug-like but also possess high **3D structural viability**.\n",
        "\n",
        "**Methodology:**\n",
        "This stage implements a policy gradient-based RL loop that directly optimizes the VAE's decoder:\n",
        "\n",
        "1.  **Encoder Freezing:** The weights of the VAE's encoder are frozen. This means only the decoder (the generative part of the model) will be updated during this phase. The model will leverage its learned understanding of chemical space from the pre-training stage while adapting its generation strategy.\n",
        "2.  **Custom Reward Function:** A sophisticated reward function is defined to score each generated molecule. This score serves as the feedback signal for the RL agent. The reward `R` is calculated based on three components:\n",
        "    * **Chemical Properties (QED & SA Score):** A primary score is calculated to favor molecules with high QED (drug-likeness) and low SA Score (ease of synthesis).\n",
        "    * **3D Embeddability (Penalty):** Crucially, the code attempts to generate a 3D conformer for each molecule using `AllChem.EmbedMolecule()`. If this process fails (returns `-1`), the molecule is considered physically unrealistic, and a **strong negative reward (penalty)** of `-1.0` is assigned. This powerfully discourages the model from generating impossible structures.\n",
        "    * **High-Quality Hits (Bonus):** Molecules that meet the stringent \"hit\" criteria (`QED > 0.6` and `SA < 5.0`) receive an **additional bonus reward**, further encouraging the model to explore the most promising regions of the chemical space.\n",
        "3.  **RL Training Loop:**\n",
        "    * At each epoch, the model **samples** a batch of new molecules.\n",
        "    * The **reward** for each valid molecule is calculated using the function described above.\n",
        "    * The model's weights are updated using a policy gradient algorithm. The loss is calculated to increase the log-probability of generating molecules that received a high reward relative to a moving average baseline (`baseline`). This stabilizes training and focuses the model on generating better-than-average compounds.\n",
        "\n",
        "**Outcome:**\n",
        "The training loop fine-tunes the model over several epochs, with the mean reward progressively increasing. This process results in a highly specialized generative model whose weights are saved to `vae_selfies_rl.pt`. This final model is optimized not just for general drug-likeness but also for producing novel molecules that are structurally and physically plausible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJDy_hRvyXwC",
        "outputId": "f011cf8d-76ce-46c4-c5ad-d498afd028c0"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# RL fine-tune (QED ↑ , SA ↓) – Compatible with Current Tokenizer/Model\n",
        "# =========================================================\n",
        "\n",
        "N_EPOCHS   = 3\n",
        "BATCH_SIZE = 256\n",
        "MAX_LEN    = 150\n",
        "LAMBDA_SA  = 0.30\n",
        "LR, CLIP   = 1e-4, 1.0\n",
        "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GAMMA      = 0.9      # baseline decay\n",
        "\n",
        "# ---------- Freeze Encoder ----------\n",
        "for p in (*model.encoder.parameters(), *model.mu.parameters(), *model.logvar.parameters()):\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
        "baseline = 0.0\n",
        "\n",
        "# ---------- Sample with Tokens ----------\n",
        "def sample_with_tokens(model, tok, n, max_len, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z   = torch.randn(n, model.mu.out_features, device=device)\n",
        "        seq = torch.full((n, 1), tok.bos, dtype=torch.long, device=device)\n",
        "        hidden = None\n",
        "        for _ in range(max_len):\n",
        "            dec_in  = torch.cat([model.emb(seq[:, -1]), z], -1).unsqueeze(1)\n",
        "            dec_out, hidden = model.decoder(dec_in, hidden)\n",
        "            logits = model.out_proj(dec_out.squeeze(1))\n",
        "            logits[:, tok.bos] = -1e9\n",
        "            nxt = torch.multinomial(torch.softmax(logits, -1), 1)\n",
        "            seq = torch.cat([seq, nxt], 1)\n",
        "            if (nxt == tok.eos).all():\n",
        "                break\n",
        "    selfies = [\"\".join(tok.idx2tok[i] for i in row[1:] if i not in (tok.pad, tok.eos)) for row in seq]\n",
        "    return selfies, seq[:, 1:]         # SMILES will be converted later\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "# ... (previous RL cell code) ...\n",
        "\n",
        "for ep in range(1, N_EPOCHS + 1):\n",
        "    sf_batch, toks_batch = sample_with_tokens(model, tok, BATCH_SIZE, MAX_LEN, DEVICE)\n",
        "\n",
        "    model.train()\n",
        "    rewards, logps = [], []\n",
        "    valid_smiles_count = 0 # To count valid molecules\n",
        "\n",
        "    for se, toks in zip(sf_batch, toks_batch):\n",
        "        smi = \"\"\n",
        "        try:\n",
        "            smi = sf.decoder(se)\n",
        "            mol = Chem.MolFromSmiles(smi)\n",
        "            if mol is None: continue\n",
        "        except sf.DecoderError:\n",
        "            continue\n",
        "\n",
        "        # --- Key modified section ---\n",
        "        mol_h = Chem.AddHs(mol)\n",
        "        # Attempt to build 3D structure\n",
        "        # embed_result = AllChem.EmbedMolecule(mol_h, AllChem.ETKDG(), randomSeed=42)\n",
        "\n",
        "        # 1. First, create a parameter object\n",
        "        params = AllChem.ETKDG()\n",
        "        # 2. Set the desired parameter (randomSeed) in it\n",
        "        params.randomSeed = 42\n",
        "        # 3. Now call the function with the modified parameter object\n",
        "        embed_result = AllChem.EmbedMolecule(mol_h, params)\n",
        "\n",
        "        R = 0.0 # Initial reward value\n",
        "        if embed_result == -1:\n",
        "            # 1. Penalty for molecules that cannot be built in 3D\n",
        "            R = -1.0\n",
        "        else:\n",
        "            # 2. Calculate reward for valid molecules\n",
        "            qed_score = QED.qed(mol)\n",
        "            sa_score = sascorer.calculateScore(mol)\n",
        "            R = 2 * qed_score - LAMBDA_SA * (sa_score / 10.0)\n",
        "\n",
        "            # 3. (Optional) Bonus for excellent hits\n",
        "            if qed_score > 0.6 and sa_score < 5.0:\n",
        "                R += 0.5\n",
        "\n",
        "        rewards.append(R)\n",
        "        valid_smiles_count += 1\n",
        "\n",
        "        # ... rest of logps calculation code ...\n",
        "        # (This section does not change)\n",
        "        inp = torch.cat([torch.tensor([tok.bos], device=DEVICE), toks[:-1].to(DEVICE)])\n",
        "        tgt = toks.to(DEVICE)\n",
        "        logits, _, _ = model(inp.unsqueeze(0), tgt.unsqueeze(0),\n",
        "                             torch.tensor([len(tgt)], device=DEVICE),\n",
        "                             tfr=0.0)\n",
        "        lp = torch.log_softmax(logits, -1)\n",
        "        seq_lp = lp.gather(2, tgt.unsqueeze(0).unsqueeze(-1)).squeeze().sum()\n",
        "        logps.append(seq_lp)\n",
        "\n",
        "    if not rewards:\n",
        "        print(f\"[RL-Ep {ep}] skip (0 valid molecules)\")\n",
        "        continue\n",
        "\n",
        "    rewards = torch.tensor(rewards, device=DEVICE)\n",
        "    logps   = torch.stack(logps)\n",
        "    baseline = GAMMA * baseline + (1 - GAMMA) * rewards.mean().item()\n",
        "    loss = -((rewards - baseline) * logps).mean()\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "    opt.step()\n",
        "\n",
        "    hits = (rewards > 0.5).float().mean()\n",
        "    # In the report, we also print the number of valid molecules\n",
        "    print(f\"[RL-Ep {ep}] meanR={rewards.mean():.3f} | hits>0.5={hits:.2%} | valid {valid_smiles_count}/{BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Final Model Checkpointing and Large-Scale Generation\n",
        "\n",
        "**Objective:**\n",
        "This final code cell serves two primary purposes: first, it saves the final, optimized state of the Reinforcement Learning model for future use and reproducibility. Second, it uses this final model to generate a large library of 10,000 new molecules, which will be the basis for the final analysis and hit selection.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Model Saving:** The state dictionary (`state_dict`) of the fine-tuned RL model, along with its associated vocabulary, is saved to the file `vae_selfies_rl.pt` using `torch.save`. This checkpoint captures the fully optimized generator.\n",
        "2.  **Model Loading:** To ensure a clean sampling process, a new instance of the VAE model (`model_rl_loaded`) is created. The saved weights from the checkpoint file are then loaded into this new model instance. The model is set to evaluation mode (`.eval()`) to disable dropout and other training-specific behaviors.\n",
        "3.  **Large-Scale Sampling:** The `sample` function is called to generate `N_SAMPLES` (10,000) new molecules from the loaded RL-tuned model. This creates a diverse library of compounds that reflect the model's optimized generative policy.\n",
        "4.  **Results Storage:** The generated molecules are processed and saved in two formats:\n",
        "    * **SMILES File:** All generated SMILES strings are written to `rl_samples_10k.smi`.\n",
        "    * **Properties CSV:** For each valid molecule, its QED, SA score, and molecular weight are calculated. These properties are compiled into a pandas DataFrame and saved to `rl_samples_props.csv`.\n",
        "\n",
        "**Outcome:**\n",
        "This cell produces the final outputs of the generative pipeline: a saved, reusable model (`vae_selfies_rl.pt`) and a comprehensive dataset (`rl_samples_props.csv`) of novel molecules and their key properties, ready for final visualization and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjbdKvRJmemt",
        "outputId": "44c0a77d-9639-4fb7-bba6-1495a6eaefbc"
      },
      "outputs": [],
      "source": [
        "# --------- 1) Save RL-fine-tuned weights -----------\n",
        "torch.save(\n",
        "    {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"vocab\": tok.idx2tok,     # For rebuilding the Tokenizer\n",
        "        \"rl_epochs\": N_EPOCHS,\n",
        "        \"lambda_sa\": LAMBDA_SA\n",
        "    },\n",
        "    \"vae_selfies_rl.pt\"\n",
        ")\n",
        "print(\"✅ Optimized model saved → vae_selfies_rl.pt\")\n",
        "\n",
        "\n",
        "# --------- 2) Sample from RL-tuned model and save results ----------\n",
        "\n",
        "SMI_OUT = \"rl_samples_10k.smi\"\n",
        "CSV_OUT = \"rl_samples_props.csv\"\n",
        "N_SAMPLES = 10000\n",
        "\n",
        "# 1. Create a new instance of the model\n",
        "#    Note: We use the 'tok' variable created in cell [11] because the vocabulary is the same\n",
        "model_rl_loaded = VAE(len(tok.idx2tok), tok.pad, tok.bos).to(DEVICE)\n",
        "\n",
        "# 2. Load the saved weights into the new model\n",
        "checkpoint = torch.load(\"vae_selfies_rl.pt\", map_location=DEVICE)\n",
        "model_rl_loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model_rl_loaded.eval()  # Important: Set the model to evaluation mode\n",
        "\n",
        "print(\"✅ Fine-tuned model successfully loaded.\")\n",
        "\n",
        "# 3. Sample using the new model (model_rl_loaded)\n",
        "print(f\"⏳ Sampling from RL model with {N_SAMPLES} samples...\")\n",
        "smiles_list = sample(model_rl_loaded, tok, n=N_SAMPLES, max_len=MAX_LEN, device=DEVICE)\n",
        "\n",
        "\n",
        "# ... rest of the code for saving to SMI and CSV files ...\n",
        "# (These sections are correct and do not need modification)\n",
        "\n",
        "# SMI file\n",
        "with open(SMI_OUT, \"w\") as f:\n",
        "    for smi in smiles_list:\n",
        "        if smi: # Only save non-empty strings\n",
        "            f.write(smi + \"\\n\")\n",
        "print(f\"✅ {len(smiles_list)} SMILES generated and saved to → {SMI_OUT}\")\n",
        "\n",
        "# CSV with QED / SA / MW\n",
        "rows = []\n",
        "valid_count = 0\n",
        "for smi in smiles_list:\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol:\n",
        "        valid_count += 1\n",
        "        rows.append(\n",
        "            {\n",
        "                \"smiles\": smi,\n",
        "                \"qed\": QED.qed(mol),\n",
        "                \"sa\": sascorer.calculateScore(mol),\n",
        "                \"mw\": Descriptors.MolWt(mol),\n",
        "            }\n",
        "        )\n",
        "pd.DataFrame(rows).to_csv(CSV_OUT, index=False)\n",
        "print(f\"✅ Properties for {valid_count} valid molecules saved to → {CSV_OUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Final Analysis: Hit Selection and Visualization of RL-Tuned Molecules\n",
        "\n",
        "**Objective:**\n",
        "This final cell analyzes the large library of molecules generated by the **Reinforcement Learning (RL) optimized model**. The goal is to identify the highest-quality \"hit\" compounds and to visualize the property distributions of the final generated chemical space. This provides a clear assessment of the success of the RL optimization strategy.\n",
        "\n",
        "**Methodology:**\n",
        "1.  **Data Loading:** The `rl_samples_props.csv` file, which contains the 10,000 molecules generated from the final RL model, is loaded into a pandas DataFrame.\n",
        "2.  **Hit Identification:** The same stringent filtering criteria used previously (QED ≥ 0.6, SA Score ≤ 5.0, MW between 150-600) are applied to this new dataset to identify the top candidates.\n",
        "3.  **Visualization:** A series of plots are generated to visualize the property distributions of the RL-tuned molecules:\n",
        "    * **QED & SA Score Histograms:** These plots show the frequency distribution for drug-likeness and synthetic accessibility. The red dashed lines indicate the filtering thresholds. These plots are critical for comparing how the RL process has shifted the distribution of generated molecules towards more desirable regions compared to the pre-trained model.\n",
        "    * **QED vs. SA Score Scatter Plot:** This plot provides a global view of the chemical property space, with the \"hit region\" clearly marked. The density and location of the points illustrate the model's success in focusing its generation on high-reward areas.\n",
        "\n",
        "**Outcome:**\n",
        "The top candidate molecules from the RL model are identified and saved to `top_hits.csv`. The visualizations clearly demonstrate the impact of the RL fine-tuning. By comparing these plots to those from the previous model, we can quantitatively and qualitatively measure the improvement and confirm that the model has successfully learned to generate molecules with more favorable drug-like properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "AeMpU_jyUmZS",
        "outputId": "35992459-8f31-4436-a86a-d48eff301659"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==============================================================\n",
        "# 1) Filter \"Hit\" Molecules + 2) Plot Distributions and Scatter\n",
        "#    (Assuming df is the DataFrame generated in the previous cell)\n",
        "# ==============================================================\n",
        "# ---------- 1. Select Hits ----------\n",
        "# Suggested criteria; adjust as desired\n",
        "QED_MIN   = 0.60          # Drug-likeness\n",
        "SA_MAX    = 5.0           # Acceptable synthetic accessibility\n",
        "MW_MIN    = 150           # Remove very light ones (often solvents or small fragments)\n",
        "MW_MAX    = 600           # According to Lipinski's Rule\n",
        "df_rl = pd.read_csv(\"rl_samples_props.csv\")\n",
        "hits = df_rl[\n",
        "    (df_rl.qed >= QED_MIN) &\n",
        "    (df_rl.sa  <= SA_MAX)  &\n",
        "    (df_rl.mw  >= MW_MIN)  &\n",
        "    (df_rl.mw  <= MW_MAX)\n",
        "].copy()\n",
        "\n",
        "print(f\"🎯  Found {len(hits)} high-quality molecules out of {len(df_rl)}  \"\n",
        "      f\"({len(hits)/len(df_rl):.1%})\")\n",
        "\n",
        "# Save Hits\n",
        "hits.to_csv(\"RL_top_hits.csv\", index=False)\n",
        "print(\"✅  Saved to RL_top_hits.csv\")\n",
        "\n",
        "\n",
        "# ---------- 2. Plot Distributions ----------\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "# --- QED Histogram ---\n",
        "plt.subplot(1,3,1)\n",
        "plt.hist(df_rl.qed, bins=40)\n",
        "plt.axvline(QED_MIN, color='red', linestyle='--')\n",
        "plt.title(\"QED distribution\")\n",
        "plt.xlabel(\"QED\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# --- SA Histogram ---\n",
        "plt.subplot(1,3,2)\n",
        "plt.hist(df_rl.sa, bins=40)\n",
        "plt.axvline(SA_MAX, color='red', linestyle='--')\n",
        "plt.title(\"SA Score distribution\")\n",
        "plt.xlabel(\"SA (lower = easier)\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# --- Scatter QED vs SA ---\n",
        "plt.subplot(1,3,3)\n",
        "plt.scatter(df_rl.qed, df_rl.sa, s=10, alpha=0.4)\n",
        "plt.axvline(QED_MIN, color='red', linestyle='--')\n",
        "plt.axhline(SA_MAX, color='red', linestyle='--')\n",
        "plt.title(\"QED vs SA\")\n",
        "plt.xlabel(\"QED (higher is better)\")\n",
        "plt.ylabel(\"SA (lower is better)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Conclusion and Future Work\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project successfully implemented an end-to-end pipeline for de novo molecular design using a deep generative model. The primary goal was to generate novel, drug-like molecules targeted at the EGFR kinase domain.\n",
        "\n",
        "The key achievements of this work include:\n",
        "* **Successful Data Pipeline:** Bioactivity data for EGFR was successfully retrieved from the ChEMBL database and preprocessed for model training.\n",
        "* **Valid Molecule Generation:** By employing a Variational Autoencoder (VAE) with the **SELFIES** molecular representation, we completely overcame the common issue of invalid SMILES generation, achieving 100% structural validity.\n",
        "* **Quality Improvement with Curriculum Learning:** The base VAE was effectively fine-tuned on a high-quality subset of drug-like molecules, successfully biasing the generator towards more favorable chemical space.\n",
        "* **Targeted Optimization with Reinforcement Learning:** The final model was optimized using an RL agent with a custom reward function. The results clearly show that this process shifted the distribution of generated molecules towards higher **QED** (drug-likeness) and lower **SA Scores** (easier synthesis).\n",
        "\n",
        "Ultimately, the project demonstrates the successful application of modern deep learning techniques to create a specialized model capable of generating novel and optimized molecules for a specific therapeutic target.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "While this project established a robust generative pipeline, several exciting avenues remain for future exploration to bring these computational designs closer to real-world drug candidates:\n",
        "\n",
        "1.  **More Extensive Training:** Re-run the Reinforcement Learning fine-tuning on more powerful hardware for a greater number of epochs. This would allow the model to better converge and likely lead to the generation of molecules with even higher reward scores.\n",
        "\n",
        "2.  **Molecular Docking Simulation:** The immediate next step is to perform molecular docking to predict the binding affinity of the top-generated hits against the EGFR protein's active site (e.g., using PDB ID `1M17`). This would provide a direct, physics-based estimate of their inhibitory potential.\n",
        "3.  **RL Optimization with Docking Score:** A more advanced approach would be to integrate the docking score directly into the Reinforcement Learning reward function. This would create a powerful feedback loop, training the model to generate molecules that are not only drug-like but also have a predicted high binding affinity for the target.\n",
        "4.  **Molecular Dynamics (MD) Simulations:** For the most promising candidates identified through docking, MD simulations could be performed to assess the stability of the ligand-protein complex over time, providing deeper insights into the binding interactions.\n",
        "5.  **Synthesis and In Vitro Assay:** The ultimate validation would be to synthesize the top computationally-ranked molecules in a laboratory and test their actual inhibitory activity against the EGFR protein through in vitro biological assays."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
