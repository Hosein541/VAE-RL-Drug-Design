# De Novo Design of EGFR Inhibitors using a VAE-RL Pipeline

This project implements an end-to-end computational pipeline for designing novel molecules targeting the Epidermal Growth Factor Receptor (EGFR) using a deep generative model. The workflow progresses from data acquisition to a final, optimized model fine-tuned with Reinforcement Learning.

---

## üìã Project Overview

The primary goal of this project is to generate novel, valid, and drug-like molecules with optimized chemical properties for inhibiting the EGFR kinase domain, a key target in cancer therapy. The pipeline leverages a Variational Autoencoder (VAE) built on the SELFIES representation to ensure 100% structural validity, and then uses Reinforcement Learning (RL) to steer the generation process towards molecules with higher drug-likeness (QED) and better synthetic accessibility (SA Score).

---

## üõ†Ô∏è Project Workflow

The project is structured in the following key stages, all implemented in the main Jupyter Notebook:

1.  **Data Acquisition:** Bioactivity data (IC50 values) for EGFR (`CHEMBL203`) is downloaded from the ChEMBL database.
2.  **Data Preprocessing:** The data is cleaned, filtered for valid entries, and the `pIC50` values are calculated to create a standardized dataset.
3.  **Base VAE Training (Curriculum Learning):** A VAE model using SELFIES is pre-trained on the entire dataset and then fine-tuned on a high-quality, drug-like subset defined by QED, SA Score, and Molecular Weight criteria.
4.  **Reinforcement Learning (RL) Optimization:** The curriculum-trained model is further optimized with an RL agent. The reward function is designed to maximize QED, minimize SA Score, and penalize molecules that are not physically plausible (i.e., cannot form a valid 3D structure).
5.  **Molecule Generation & Analysis:** Both the curriculum-trained model and the final RL-tuned model are used to generate large libraries of novel molecules, which are then analyzed and compared.

---

## üìà Results: Impact of Reinforcement Learning

The Reinforcement Learning step significantly improved the quality of the generated molecules. The following plots compare the property distributions of molecules generated by the model **before** and **after** RL fine-tuning.

**Note:** Due to computational constraints (training on Google Colab's free tier), the RL fine-tuning was run for only a limited number of epochs (N=3). While clear improvements are visible, more extensive training would likely yield even better results.

#### Before RL (Curriculum-Trained Model Only)
The base model explores a broad chemical space, but the bulk of the generated molecules have suboptimal properties.
* **Hits Found:** 136 / 4995 (**2.7%**)

![Before RL](image_3e3d14.png)

#### After RL Fine-Tuning
The RL-tuned model clearly shifts its generative distribution towards more desirable regions (higher QED, lower SA Score), even with limited training.
* **Hits Found:** 258 / 9992 (**2.6%**)

![After RL](image_771a9d.png)

As seen in the charts, the RL-tuned model produces molecules that are demonstrably more drug-like and synthetically accessible, confirming the success of the optimization strategy.

---

## üöÄ How to Run

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/Hosein541/VAE-RL-Drug-Design
    cd VAE-RL-Drug-Design
    ```
2.  **Open the notebook:**
    * The main file is `Drug_Creation.ipynb`. It is recommended to run it in a GPU-enabled environment like Google Colab.
3.  **Run all cells:**
    * Execute the cells sequentially. The notebook will handle all library installations, data downloads, model training, and result generation.

---

## üî¨ Future Work

While the project established a robust generative pipeline, the following steps could further validate and improve the results:

* **More Extensive Training:** Re-run the Reinforcement Learning fine-tuning on more powerful hardware for a greater number of epochs. This would allow the model to better converge and likely lead to the generation of molecules with even higher reward scores.
* **Molecular Docking Simulation:** Perform molecular docking to predict the binding affinity of the top-generated hits against the EGFR protein's active site (e.g., using PDB ID `1M17`).
* **RL Optimization with Docking Score:** Integrate the docking score directly into the RL reward function to create a powerful feedback loop that optimizes for binding affinity.
* **Molecular Dynamics (MD) Simulations:** Assess the stability of the top ligand-protein complexes over time.
* **In Vitro Validation:** Synthesize the most promising computational hits for experimental validation in a laboratory setting.
